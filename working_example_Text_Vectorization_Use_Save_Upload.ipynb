{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w47A5xPdXp1W"
      },
      "source": [
        "# KERAS TEXT VECTORIZATION LAYER: USE, SAVE, AND UPLOAD\n",
        "\n",
        "**Author:** [Murat Karakaya](https://www.linkedin.com/in/muratkarakaya/)<br>\n",
        "**Date created:** 05 Oct 2021<br>\n",
        "**Last modified:** 24 Oct 2021<br>\n",
        "**Description:** This is a new part of the \"**[tf.keras.layers: Understand & Use](https://www.youtube.com/playlist?list=PLQflnv_s49v_7WIgOo9mVKptLZHyOYysD)**\" / \"[**tf.keras.layers: Anla ve Kullan**](https://www.youtube.com/playlist?list=PLQflnv_s49v9h85zD1_GDfTxZOrCWTDhp)\" series. In this part, we will build, adapt, use, save, and upload the Keras TextVectorization layer. \n",
        "\n",
        "We will download a [Kaggle Dataset](https://www.kaggle.com/savasy/multiclass-classification-data-for-turkish-tc32?select=ticaret-yorum.csv) in which there are 32 topics and more than 400K total reviews. \n",
        "In this tutorial, we will use this dataset for a multi class text classification task.\n",
        "\n",
        "Our **main aim** is to learn how to efectively use the Keras `TextVectorization` layer in practice.\n",
        "\n",
        "The tutorial has 5 parts:\n",
        "\n",
        "* **PART A: BACKGROUND**\n",
        "* **PART B: KNOW THE DATA**\n",
        "* **PART C: USE KERAS TEXT VECTORIZATION LAYER**\n",
        "* **PART D: BUILD AN END-TO-END MODEL**\n",
        "* **PART E: SUMMARY**\n",
        "\n",
        "\n",
        "At the end of this tutorial, we will cover:\n",
        "* What a Keras `TextVectorization` layer is\n",
        "* Why we need to use a Keras `TextVectorization` layer in Natural Languge Processing (NLP) tasks\n",
        "* How to employ a Keras `TextVectorization` layer in **Text Preprocessing**\n",
        "* How to integrate a Keras `TextVectorization` layer to a trained model\n",
        "* How to save and upload a Keras `TextVectorization` layer and a model with a Keras `TextVectorization` layer\n",
        "* How to integrate a Keras `TextVectorization` layer with **TensorFlow Data Pipeline** API (`tf.data`)\n",
        "* How to design, train, save, and load an End-to-End model using Keras `TextVectorization` layer\n",
        "\n",
        "**Accessible on:**\n",
        "* [YouTube in English](https://youtube.com/playlist?list=PLQflnv_s49v8Eo2idw9Ju5Qq3JTEF-OFW)\n",
        "* [YouTube in Turkish](https://youtube.com/playlist?list=PLQflnv_s49v8-xeTLx1QmuE-YkRB4bToF)\n",
        "* [Medium](https://kmkarakaya.medium.com/text-vectorization-use-save-upload-54d65945d222)\n",
        "* [Github pages](https://kmkarakaya.github.io/Deep-Learning-Tutorials/)\n",
        "* [Github Repo](https://github.com/kmkarakaya/Deep-Learning-Tutorials)\n",
        "* [Google Colab](https://colab.research.google.com/drive/1_hiUXcX6DwGEsPP2iE7i-HAs-5HqQrSe?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTmjxD3k10sj"
      },
      "source": [
        "# REFERENCES\n",
        "* [Keras Preprocessing layers by Keras.io](https://keras.io/api/layers/preprocessing_layers/)\n",
        "* [Text classification from scratch by Keras.io](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
        "* [TextVectorization layer by Keras.io](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NHPGuvcdm3"
      },
      "source": [
        "# **PART A: BACKGROUND**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXnOwUb7oO0-"
      },
      "source": [
        "# 1 TERMS & CONCEPTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3wsbjvlW5bk"
      },
      "source": [
        "## 1.1 What is Text Vectorization?\n",
        "\n",
        "Text Vectorization is the process of converting text into numerical representation. \n",
        "\n",
        "There are many different techniques proposed to convert text to a numerical form such as:\n",
        "* One-hot Encoding (OHE)\n",
        "* Count Vectorizer\n",
        "* Bag-of-Words (BOW)\n",
        "* N-grams\n",
        "* Term Frequency\n",
        "* Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "* Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4cW26J5YnIR"
      },
      "source": [
        "## 1.2. What is Text Preprocessing?\n",
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more suitable form so that Machine Learning or Deep Learning algorithms can perform better.\n",
        "\n",
        "The main phases of Text preprocessing:\n",
        "* **Noise Removal** (cleaning) – Removing unnecessary characters and formatting\n",
        "* **Tokenization** – break multi-word strings into smaller components\n",
        "* **Normalization** – a catch-all term for processing data; this includes stemming and lemmatization\n",
        "\n",
        "\n",
        "Some of the common **Noise Removal** (cleaning) steps are:\n",
        "\n",
        "* Removal of Punctuations\n",
        "* Removal of Frequent words\n",
        "* Removal of Rare words\n",
        "* Removal of emojis\n",
        "* Removal of emoticons\n",
        "* Conversion of emoticons to words\n",
        "* Conversion of emojis to words\n",
        "* Removal of URLs\n",
        "* Removal of HTML tags\n",
        "* Chat words conversion\n",
        "* Spelling correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6LfU5Muatns"
      },
      "source": [
        "**Tokenization** is about splitting strings of text into smaller pieces, or “tokens”. Paragraphs can be tokenized into sentences and sentences can be tokenized into words. \n",
        "\n",
        "\n",
        "**Noise Removal** and **Tokenization** and  are staples of almost all text pre-processing pipelines. However, some data may require further processing through text **normalization**. Some of the common **normalization** steps are:\n",
        "* Upper or lowercasing\n",
        "* Stopword removal\n",
        "* Stemming – bluntly removing prefixes and suffixes from a word\n",
        "* Lemmatization – replacing a single-word token with its root\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIVN5X4Di96"
      },
      "source": [
        "## 1.3. What is Keras Text Vectorization layer?\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKTqWoHWD0_a"
      },
      "source": [
        "`tf.keras.layers.TextVectorization` layer is one of the [Keras Preprocessing layers](https://keras.io/guides/preprocessing_layers/). \n",
        "\n",
        "We can preproces the input by using different libraries such as Python String library, or SciKit Learn library, etc. \n",
        "\n",
        "However, there are very important advantages using the [Keras Preprocessing layers](https://keras.io/guides/preprocessing_layers/):\n",
        "\n",
        "* You can build **Keras-native** input processing **pipelines**. These input processing pipelines can be used as **independent** preprocessing code in **non-Keras workflows**, combined directly with Keras models, and exported as part of a Keras SavedModel.\n",
        "\n",
        "* You can build and **export** models that are **truly end-to-end**: models that accept **raw data** (images or raw structured data) as input; models that handle feature **normalization** or feature value **indexing** on their own.\n",
        "\n",
        "Today, we will deal with the `tf.keras.layers.TextVectorization` layer which:\n",
        "* turns ***raw strings*** into an **encoded representation** \n",
        "* that representation can be read by an `Embedding` layer or `Dense` layer.\n",
        "\n",
        "That is, the `tf.keras.layers.TextVectorization` layer can be used in \n",
        "* **Text Preprocessing** and\n",
        "* **Text Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKitY3ahAx7"
      },
      "source": [
        "# 2. IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6TwpteGR7R"
      },
      "source": [
        "**IMPORTANT:** When I prepared this tutorial on 05 Oct 2021, the current version (2.6.0) of TF and Keras generate some **errors** in saving and uploading the **tf.keras.layers.TextVectorization layer**. \n",
        "\n",
        "However, the nightly version has no problem handling these operations.\n",
        "\n",
        "For more information about the bug, please see [here](https://github.com/keras-team/keras/issues/15443#issuecomment-938211510)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw5YiGjuH9pN"
      },
      "source": [
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"tf version:\",tf.__version__)\n",
        "\n",
        "print(\"keras version:\", keras.__version__)\n",
        "\n",
        "tf version: 2.6.0\n",
        "\n",
        "keras version: 2.6.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAjNqIse9pJ"
      },
      "source": [
        "Therefore, below I first upload the TF nightly version. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5bmtFNee07h"
      },
      "source": [
        "```python\n",
        "tf version: 2.8.0-dev20211005\n",
        "keras version: 2.7.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPRv2g92Polb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7ca4d4-12c1-43c7-a0bc-036a1aa4f0bb"
      },
      "source": [
        "pip install tf-nightly --quiet --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.76 s (started: 2022-10-04 09:14:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsDBX1yETA3v"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RqP1sQpRhf5",
        "outputId": "ded9369b-d299-4199-d767-5ada84bd142a"
      },
      "source": [
        "print(\"tf version:\",tf.__version__)\n",
        "print(\"keras version:\", keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf version: 2.8.2\n",
            "keras version: 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPDDB871W56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b3e52e-a22c-4e87-dfeb-c5ebbab667b8"
      },
      "source": [
        "#@title Record Each Cell's Execution Time\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (7.9.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 36.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.18.1\n",
            "time: 365 µs (started: 2022-10-04 10:36:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcyE0LzKL-4"
      },
      "source": [
        "# 3. DOWNLOAD A KAGGLE DATASET INTO GOOGLE COLAB\n",
        "\n",
        "The [Multi Class Classification Dataset for Turkish](https://www.kaggle.com/savasy/multiclass-classification-data-for-turkish-tc32?select=ticaret-yorum.csv) is a **benchmark dataset for Turkish** **text classification** task. \n",
        "\n",
        "It contians 430K comments/reviews for a total 32 categories products or services.\n",
        "\n",
        "Each category roughly has 13K comments.\n",
        "\n",
        "A baseline algoritm, Naive Bayes, gets %84 F1 score.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[My blog post explaning how to download Kaggle Datasets is here.](https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a)\n",
        "\n",
        "My video tutorial explaning how to download Kaggle Datasets is here: [Turkish](https://youtu.be/ls47CPFU1vE)/[English](https://youtu.be/_rlt4mzLDLc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pIhSabUHQPB",
        "outputId": "48d7995b-4be8-409b-a391-ae9823311936"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "time: 33.9 s (started: 2022-10-04 10:36:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyAOsTtRIaSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131bf394-6355-49ee-dab9-8641f1cb573d"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/kaggle\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 524 µs (started: 2022-10-04 10:37:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGE3TaZtIsIl",
        "outputId": "b2fea235-3ae6-4d77-d11d-ec7cb1ff4175"
      },
      "source": [
        "#changing the working directory\n",
        "%cd \"/content/gdrive/MyDrive/kaggle\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/kaggle\n",
            "time: 6.21 ms (started: 2022-10-04 10:37:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o__jkIwI-63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbb0678-db39-42d2-9273-eaa0ddc3cd04"
      },
      "source": [
        "#get the api command from kaggle dataset page\n",
        "!kaggle datasets download -d savasy/multiclass-classification-data-for-turkish-tc32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading multiclass-classification-data-for-turkish-tc32.zip to /content/gdrive/MyDrive/kaggle\n",
            " 96% 49.0M/51.3M [00:08<00:00, 8.24MB/s]\n",
            "100% 51.3M/51.3M [00:08<00:00, 6.49MB/s]\n",
            "time: 11.5 s (started: 2022-10-04 10:37:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cto_q1OUKQ9",
        "outputId": "b685b7b9-4b12-48f6-9f4d-800c7186773c"
      },
      "source": [
        "# check the downloaded zip file\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_to_category.pkl\t\t\t\t     stop_words_turkish.txt\n",
            "kaggle.json\t\t\t\t\t     ticaret-yorum.csv\n",
            "multiclass-classification-data-for-turkish-tc32.zip  vectorize_layer_model\n",
            "time: 128 ms (started: 2022-10-04 10:37:34 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViTkX2CbLTUe",
        "outputId": "e421a013-4202-4ec0-b1b3-1abebcb78167"
      },
      "source": [
        "# unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  multiclass-classification-data-for-turkish-tc32.zip\n",
            "replace ticaret-yorum.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ticaret-yorum.csv       \n",
            "time: 8.86 s (started: 2022-10-04 10:37:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvogE3ztLndZ",
        "outputId": "312b290e-24e5-4a95-db6a-e36cb98a10a6"
      },
      "source": [
        "# check the downloaded csv file\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_to_category.pkl  stop_words_turkish.txt  vectorize_layer_model\n",
            "kaggle.json\t    ticaret-yorum.csv\n",
            "time: 122 ms (started: 2022-10-04 10:37:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeJakYWhG9W1"
      },
      "source": [
        "# 4. LOAD STOP WORDS IN TURKISH\n",
        "\n",
        "As you might know \"**Stop words**\" are a set of commonly used words in a language. Examples of stop words in **English** are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to **eliminate** words that are so commonly used that they carry **very little useful information**.\n",
        "\n",
        "I begin with uploading an existing  list of stop words in Turkish below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y0t0dhJG77-",
        "outputId": "e3d43d4c-c3bb-4de8-cdf1-3c782d60fe89"
      },
      "source": [
        "tr_stop_words = pd.read_csv('stop_words_turkish.txt',header=None)\n",
        "for each in tr_stop_words.values[:5]:\n",
        "  print(each[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ama\n",
            "amma\n",
            "anca\n",
            "ancak\n",
            "belki\n",
            "time: 687 ms (started: 2022-10-04 10:37:54 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WQNplaQhmB3"
      },
      "source": [
        "# 5. LOAD THE DATASET\n",
        "After downloading the dataset from Kaggle website, we can upload it by using the Pandas library `read_csv()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v0as4QesGQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f77816e-0011-4aab-8f14-dffe0c4611ff"
      },
      "source": [
        "data = pd.read_csv('ticaret-yorum.csv')\n",
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.88 s (started: 2022-10-04 10:38:00 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHr3M_p1ctKe"
      },
      "source": [
        "# **PART B: KNOW THE DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVGStvwEesYR"
      },
      "source": [
        "# 6. EXPLORE THE DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C0H8DoiguLw"
      },
      "source": [
        "Before getting into the details of how to use the `tf.keras.layers.TextVectorization` layer, let me introduce the dataset briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-eRYB-PlQsx"
      },
      "source": [
        "## Shuffle Data\n",
        "\n",
        "It is a really good and useful habit that, before doing anything else, as a first step in the preprocessing shuffle the data!\n",
        "\n",
        "Actually, I will shuffle the data at the last step of the pipeline.\n",
        "But it does not hurt shuffling it twice :))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAIVvhLjlvYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33a0513-22fa-4767-a848-66c9defe013f"
      },
      "source": [
        "data= data.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 85.8 ms (started: 2022-10-04 10:38:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UOqnurdk-B"
      },
      "source": [
        "## Summary Information about the dataset\n",
        "\n",
        "Get the initial information about the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2H1RqPks2tL",
        "outputId": "5f446e6e-93ab-436a-abeb-69516065447a"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 431306 entries, 306753 to 27112\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   category  431306 non-null  object\n",
            " 1   text      431306 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 9.9+ MB\n",
            "time: 113 ms (started: 2022-10-04 10:38:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynP1CD8oVtv_"
      },
      "source": [
        "We have a total of **431306** of rows and **2** columns: ***category*** & ***text***.\n",
        "\n",
        "According to `data.info()`, there is **no null values** in the dataset. If there are any null values in the dataset, we could drop these null values as follows:\n",
        "```python\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.isnull().sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31JouHmqVtCm"
      },
      "source": [
        "## Sample Reviews and their categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rzfkiUERVz5O",
        "outputId": "94489e20-8dc9-41be-e339-bbc205f35031"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    category  \\\n",
              "306753   mobilya-ev-tekstili   \n",
              "55080   cep-telefon-kategori   \n",
              "109762                enerji   \n",
              "354507                saglik   \n",
              "320925  mucevher-saat-gozluk   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                         text  \n",
              "306753                      Devrin Mobilya İnegöl Mobilya Arızası,20019'da yatak odası takımı aldım yaklaşık 7 ay sonra kapılar arızalı olduğu için aradım 4 ay sonra geldiler ama tamam dediler olmadı aynı arıza tekrar devam etti aradım 1 ay sonra geldiler yine olmadı gardolabı kullanamıyorum artık evde de huzursuz olduk artık şimdi hakkımı hukuki yollardan arayaca...Devamını oku  \n",
              "55080                                                iPhone Onarım Merkezi Sorun Yok Denilmesi,İPhone 7 Plus cihazımı garanti süresine 3 gün kala servise gönderdim. Servis Apple onarım merkezine yönlendirildi. Üründe bir hata olmadığını iade edileceği söylendi. Üründe hata olmasa servis onarım merkezine yönlendirmez. Garanti süresine az kaldı diye fırsat güdüyorlar. Devamını oku  \n",
              "109762                                                                                                                                                                                             Gediz Elektrik Perakende Şiddetli Elektrik Kesintisi Sorunu,\"Lütfen, lütfen kıymetli bakanım sesimi duyar mısınız. Manisa şehzadeler, tilki süleymaniye mahallesinden size sesleniyorum...  \n",
              "354507  Özel Körfez Marmara Hastanesi'ne Telefonla Ulaşım Sorunu!,Hastaneye tahlil sonuçları için verdikleri numaralara kesinlikle bakan yok yemek saatleri haricinde bile aradığım numaralara kesinlikle ulaşım sağlayamıyorum. Telefonlar kaldırılıp kapatılıyor ya da boşta bekletiliyor. Acil bir şey için doktorumuzu arasak ya da santralleri kesinlikle telefon başında...Devamını oku  \n",
              "320925                                Atasun Optik Mağdur Etti!,\"Denizli Forum AVM Atasun Optik'den 29.07.18 tarihinde 2 farklı kişi için 2 adet Osse marka güneş gözlüğü aldık. Benim aldığım güneş gözlüğü başımda ve gözümde ağrı yaptığı için 31.07.18 tarihinde Aydın Forum AVM şubesinden fazla para vererek Rayban marka güneş gözlüğü aldım, kardeşime almış olduğumu...Devamını oku\"  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a320ba8-af1f-4996-bff3-ee1a31fa5d16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>306753</th>\n",
              "      <td>mobilya-ev-tekstili</td>\n",
              "      <td>Devrin Mobilya İnegöl Mobilya Arızası,20019'da yatak odası takımı aldım yaklaşık 7 ay sonra kapılar arızalı olduğu için aradım 4 ay sonra geldiler ama tamam dediler olmadı aynı arıza tekrar devam etti aradım 1 ay sonra geldiler yine olmadı gardolabı kullanamıyorum artık evde de huzursuz olduk artık şimdi hakkımı hukuki yollardan arayaca...Devamını oku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55080</th>\n",
              "      <td>cep-telefon-kategori</td>\n",
              "      <td>iPhone Onarım Merkezi Sorun Yok Denilmesi,İPhone 7 Plus cihazımı garanti süresine 3 gün kala servise gönderdim. Servis Apple onarım merkezine yönlendirildi. Üründe bir hata olmadığını iade edileceği söylendi. Üründe hata olmasa servis onarım merkezine yönlendirmez. Garanti süresine az kaldı diye fırsat güdüyorlar. Devamını oku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109762</th>\n",
              "      <td>enerji</td>\n",
              "      <td>Gediz Elektrik Perakende Şiddetli Elektrik Kesintisi Sorunu,\"Lütfen, lütfen kıymetli bakanım sesimi duyar mısınız. Manisa şehzadeler, tilki süleymaniye mahallesinden size sesleniyorum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354507</th>\n",
              "      <td>saglik</td>\n",
              "      <td>Özel Körfez Marmara Hastanesi'ne Telefonla Ulaşım Sorunu!,Hastaneye tahlil sonuçları için verdikleri numaralara kesinlikle bakan yok yemek saatleri haricinde bile aradığım numaralara kesinlikle ulaşım sağlayamıyorum. Telefonlar kaldırılıp kapatılıyor ya da boşta bekletiliyor. Acil bir şey için doktorumuzu arasak ya da santralleri kesinlikle telefon başında...Devamını oku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320925</th>\n",
              "      <td>mucevher-saat-gozluk</td>\n",
              "      <td>Atasun Optik Mağdur Etti!,\"Denizli Forum AVM Atasun Optik'den 29.07.18 tarihinde 2 farklı kişi için 2 adet Osse marka güneş gözlüğü aldık. Benim aldığım güneş gözlüğü başımda ve gözümde ağrı yaptığı için 31.07.18 tarihinde Aydın Forum AVM şubesinden fazla para vererek Rayban marka güneş gözlüğü aldım, kardeşime almış olduğumu...Devamını oku\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a320ba8-af1f-4996-bff3-ee1a31fa5d16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a320ba8-af1f-4996-bff3-ee1a31fa5d16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a320ba8-af1f-4996-bff3-ee1a31fa5d16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14 ms (started: 2022-10-04 10:38:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqjl4QZHvAxK"
      },
      "source": [
        "# 7. CREATE A TENSORFLOW DATA PIPELINE FOR TEXT PREPROCESSING &  VECTORIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU26jUANffDY"
      },
      "source": [
        "So far, we just observe some properties of the **raw data**.\n",
        "Using these observations, we are ready to preprocess the `text` data for a classifier model.\n",
        "\n",
        "Below, we will begin to create a **TensorFlow data pipeline** which includes **Keras Text Vectorization layer** for preprocessing the data and preparing it for a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfxrJ2-Agb7m"
      },
      "source": [
        "A pipeline for a text model mostly involves extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths.\n",
        "\n",
        "In this tutorial, I will use the TensorFlow \"**tf.data**\" API. If you are not familiar with TF data pipeline \"**tf.data**\" API, you can apply below resources:\n",
        "* Official TensorFlow blog: [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) \n",
        "* The Murat Karakaya Akademi YouTube playlist in Turkish: [tf.data: TensorFlow Data Pipeline Anlamak ve Kullanmak](https://www.youtube.com/playlist?list=PLQflnv_s49v8l8dYU01150vcoAn4sWSAm)  \n",
        "* The Murat Karakaya Akademi YouTube playlist in English:[TensorFlow Data Pipeline: How to Design Code Use TensorFlow Data Pipelines with Python & Keras](https://www.youtube.com/playlist?list=PLQflnv_s49v_m6KLMsORgs9hVIvDCwDAb)\n",
        "* The Murat Karakaya Akademi Medium blog: [tf.data: Tensorflow Data Pipelines](https://medium.com/deep-learning-with-keras/tf-data-tensorflow-data-pipelines-71915155bdf2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVSPJfBxlmN8"
      },
      "source": [
        "## Convert Categories From Strings to Integer Ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhGRDmGZjhBt"
      },
      "source": [
        "Observe that the categories (topics/class)of the reviews are **strings**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytzcLGSHbKrw",
        "outputId": "b312f21b-d3e9-40cd-c5f7-0915a22e5a7b"
      },
      "source": [
        "data[\"category\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306753     mobilya-ev-tekstili\n",
              "55080     cep-telefon-kategori\n",
              "109762                  enerji\n",
              "354507                  saglik\n",
              "320925    mucevher-saat-gozluk\n",
              "                  ...         \n",
              "197164                  icecek\n",
              "5280                 alisveris\n",
              "188956                  icecek\n",
              "419386                  ulasim\n",
              "27112               beyaz-esya\n",
              "Name: category, Length: 431306, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.57 ms (started: 2022-10-04 10:38:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-2z9GHNiA8Y"
      },
      "source": [
        "We nned to create **integer** category **ids** from **string** category **names** by adding a new column to the dataframe \"**category_id**\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "7n8IFrNeiU7W",
        "outputId": "bd4493ac-2f93-43ec-c8d4-c091f61fd639"
      },
      "source": [
        "data[\"category\"] = data[\"category\"].astype('category')\n",
        "data[\"category_id\"] = data[\"category\"].cat.codes\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    category  \\\n",
              "306753   mobilya-ev-tekstili   \n",
              "55080   cep-telefon-kategori   \n",
              "109762                enerji   \n",
              "354507                saglik   \n",
              "320925  mucevher-saat-gozluk   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
              "306753                      Devrin Mobilya İnegöl Mobilya Arızası,20019'da yatak odası takımı aldım yaklaşık 7 ay sonra kapılar arızalı olduğu için aradım 4 ay sonra geldiler ama tamam dediler olmadı aynı arıza tekrar devam etti aradım 1 ay sonra geldiler yine olmadı gardolabı kullanamıyorum artık evde de huzursuz olduk artık şimdi hakkımı hukuki yollardan arayaca...Devamını oku   \n",
              "55080                                                iPhone Onarım Merkezi Sorun Yok Denilmesi,İPhone 7 Plus cihazımı garanti süresine 3 gün kala servise gönderdim. Servis Apple onarım merkezine yönlendirildi. Üründe bir hata olmadığını iade edileceği söylendi. Üründe hata olmasa servis onarım merkezine yönlendirmez. Garanti süresine az kaldı diye fırsat güdüyorlar. Devamını oku   \n",
              "109762                                                                                                                                                                                             Gediz Elektrik Perakende Şiddetli Elektrik Kesintisi Sorunu,\"Lütfen, lütfen kıymetli bakanım sesimi duyar mısınız. Manisa şehzadeler, tilki süleymaniye mahallesinden size sesleniyorum...   \n",
              "354507  Özel Körfez Marmara Hastanesi'ne Telefonla Ulaşım Sorunu!,Hastaneye tahlil sonuçları için verdikleri numaralara kesinlikle bakan yok yemek saatleri haricinde bile aradığım numaralara kesinlikle ulaşım sağlayamıyorum. Telefonlar kaldırılıp kapatılıyor ya da boşta bekletiliyor. Acil bir şey için doktorumuzu arasak ya da santralleri kesinlikle telefon başında...Devamını oku   \n",
              "320925                                Atasun Optik Mağdur Etti!,\"Denizli Forum AVM Atasun Optik'den 29.07.18 tarihinde 2 farklı kişi için 2 adet Osse marka güneş gözlüğü aldık. Benim aldığım güneş gözlüğü başımda ve gözümde ağrı yaptığı için 31.07.18 tarihinde Aydın Forum AVM şubesinden fazla para vererek Rayban marka güneş gözlüğü aldım, kardeşime almış olduğumu...Devamını oku\"   \n",
              "\n",
              "        category_id  \n",
              "306753           22  \n",
              "55080             4  \n",
              "109762            8  \n",
              "354507           26  \n",
              "320925           23  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d102c15b-0511-45fa-a6eb-cdea53376d25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>306753</th>\n",
              "      <td>mobilya-ev-tekstili</td>\n",
              "      <td>Devrin Mobilya İnegöl Mobilya Arızası,20019'da yatak odası takımı aldım yaklaşık 7 ay sonra kapılar arızalı olduğu için aradım 4 ay sonra geldiler ama tamam dediler olmadı aynı arıza tekrar devam etti aradım 1 ay sonra geldiler yine olmadı gardolabı kullanamıyorum artık evde de huzursuz olduk artık şimdi hakkımı hukuki yollardan arayaca...Devamını oku</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55080</th>\n",
              "      <td>cep-telefon-kategori</td>\n",
              "      <td>iPhone Onarım Merkezi Sorun Yok Denilmesi,İPhone 7 Plus cihazımı garanti süresine 3 gün kala servise gönderdim. Servis Apple onarım merkezine yönlendirildi. Üründe bir hata olmadığını iade edileceği söylendi. Üründe hata olmasa servis onarım merkezine yönlendirmez. Garanti süresine az kaldı diye fırsat güdüyorlar. Devamını oku</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109762</th>\n",
              "      <td>enerji</td>\n",
              "      <td>Gediz Elektrik Perakende Şiddetli Elektrik Kesintisi Sorunu,\"Lütfen, lütfen kıymetli bakanım sesimi duyar mısınız. Manisa şehzadeler, tilki süleymaniye mahallesinden size sesleniyorum...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354507</th>\n",
              "      <td>saglik</td>\n",
              "      <td>Özel Körfez Marmara Hastanesi'ne Telefonla Ulaşım Sorunu!,Hastaneye tahlil sonuçları için verdikleri numaralara kesinlikle bakan yok yemek saatleri haricinde bile aradığım numaralara kesinlikle ulaşım sağlayamıyorum. Telefonlar kaldırılıp kapatılıyor ya da boşta bekletiliyor. Acil bir şey için doktorumuzu arasak ya da santralleri kesinlikle telefon başında...Devamını oku</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320925</th>\n",
              "      <td>mucevher-saat-gozluk</td>\n",
              "      <td>Atasun Optik Mağdur Etti!,\"Denizli Forum AVM Atasun Optik'den 29.07.18 tarihinde 2 farklı kişi için 2 adet Osse marka güneş gözlüğü aldık. Benim aldığım güneş gözlüğü başımda ve gözümde ağrı yaptığı için 31.07.18 tarihinde Aydın Forum AVM şubesinden fazla para vererek Rayban marka güneş gözlüğü aldım, kardeşime almış olduğumu...Devamını oku\"</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d102c15b-0511-45fa-a6eb-cdea53376d25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d102c15b-0511-45fa-a6eb-cdea53376d25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d102c15b-0511-45fa-a6eb-cdea53376d25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 53.2 ms (started: 2022-10-04 10:38:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4MqVsZlix51"
      },
      "source": [
        "Lastly, we can check the number of categories. Note that it should be **32**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsaO4v8Zj34I",
        "outputId": "923f7de6-e19d-4644-8b42-19749d2551df"
      },
      "source": [
        "data['category']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306753     mobilya-ev-tekstili\n",
              "55080     cep-telefon-kategori\n",
              "109762                  enerji\n",
              "354507                  saglik\n",
              "320925    mucevher-saat-gozluk\n",
              "                  ...         \n",
              "197164                  icecek\n",
              "5280                 alisveris\n",
              "188956                  icecek\n",
              "419386                  ulasim\n",
              "27112               beyaz-esya\n",
              "Name: category, Length: 431306, dtype: category\n",
              "Categories (32, object): ['alisveris', 'anne-bebek', 'beyaz-esya', 'bilgisayar', ..., 'spor',\n",
              "                          'temizlik', 'turizm', 'ulasim']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.28 ms (started: 2022-10-04 10:38:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHdF7xZguBJy"
      },
      "source": [
        "## Build a Dictionary for id to text category (topic) look-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfM2IVJuDJx",
        "outputId": "9b9ba83b-f4cd-47bb-b74c-796bffea9506"
      },
      "source": [
        "id_to_category = pd.Series(data.category.values,index=data.category_id).to_dict()\n",
        "id_to_category"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{22: 'mobilya-ev-tekstili',\n",
              " 4: 'cep-telefon-kategori',\n",
              " 8: 'enerji',\n",
              " 26: 'saglik',\n",
              " 23: 'mucevher-saat-gozluk',\n",
              " 27: 'sigortacilik',\n",
              " 20: 'medya',\n",
              " 13: 'hizmet-sektoru',\n",
              " 29: 'temizlik',\n",
              " 15: 'internet',\n",
              " 30: 'turizm',\n",
              " 14: 'icecek',\n",
              " 5: 'egitim',\n",
              " 9: 'etkinlik-ve-organizasyon',\n",
              " 3: 'bilgisayar',\n",
              " 21: 'mekan-ve-eglence',\n",
              " 2: 'beyaz-esya',\n",
              " 18: 'kisisel-bakim-ve-kozmetik',\n",
              " 16: 'kamu-hizmetleri',\n",
              " 6: 'elektronik',\n",
              " 24: 'mutfak-arac-gerec',\n",
              " 0: 'alisveris',\n",
              " 28: 'spor',\n",
              " 10: 'finans',\n",
              " 1: 'anne-bebek',\n",
              " 11: 'gida',\n",
              " 19: 'kucuk-ev-aletleri',\n",
              " 7: 'emlak-ve-insaat',\n",
              " 25: 'otomotiv',\n",
              " 17: 'kargo-nakliyat',\n",
              " 12: 'giyim',\n",
              " 31: 'ulasim'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 271 ms (started: 2022-10-04 10:38:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AYZ7g0u1tEnt",
        "outputId": "aaa91cca-959a-4b2d-e84f-ed4c7b976a4f"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/kaggle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.53 ms (started: 2022-10-04 10:38:15 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnxVEIVctDHz",
        "outputId": "ab451832-0074-47f2-d079-7c35ecad825e"
      },
      "source": [
        "import pickle\n",
        "pkl_file = open(\"id_to_category.pkl\", \"wb\")\n",
        "pickle.dump(id_to_category, pkl_file)\n",
        "pkl_file.close()\n",
        "\n",
        "pkl_file = open(\"id_to_category.pkl\", \"rb\")\n",
        "uploaded_id_to_category = pickle.load(pkl_file)\n",
        "print(uploaded_id_to_category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{22: 'mobilya-ev-tekstili', 4: 'cep-telefon-kategori', 8: 'enerji', 26: 'saglik', 23: 'mucevher-saat-gozluk', 27: 'sigortacilik', 20: 'medya', 13: 'hizmet-sektoru', 29: 'temizlik', 15: 'internet', 30: 'turizm', 14: 'icecek', 5: 'egitim', 9: 'etkinlik-ve-organizasyon', 3: 'bilgisayar', 21: 'mekan-ve-eglence', 2: 'beyaz-esya', 18: 'kisisel-bakim-ve-kozmetik', 16: 'kamu-hizmetleri', 6: 'elektronik', 24: 'mutfak-arac-gerec', 0: 'alisveris', 28: 'spor', 10: 'finans', 1: 'anne-bebek', 11: 'gida', 19: 'kucuk-ev-aletleri', 7: 'emlak-ve-insaat', 25: 'otomotiv', 17: 'kargo-nakliyat', 12: 'giyim', 31: 'ulasim'}\n",
            "time: 837 ms (started: 2022-10-04 10:38:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aym0dhZz-byL"
      },
      "source": [
        "## Reduce the Size of the Dataset\n",
        "\n",
        "Since using a large dataset for **testing** your pipeline would take more time, you would prefer **take a portion** of the raw dataset as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhQjlJ9CCbO0",
        "outputId": "4b3b0e85-7298-4c74-89d9-3585339dfc4c"
      },
      "source": [
        "#limit the number of samples to be used in testing the pipeline\n",
        "#data_size= 1000 #instead of 431306 \n",
        "#data= data[:data_size]\n",
        "#data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 406 µs (started: 2022-10-04 10:38:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9KpOpKnMk7"
      },
      "source": [
        "## Split the Raw Dataset into Train and Test Datasets\n",
        "\n",
        "To prevent **data leakage** during preprocessing the text data, we need to split the text int Train and Test data sets. \n",
        "\n",
        "**Data leakage** refers to a mistake make by the creator of a machine learning model in which they accidentally share information between the test and training data-sets. Typically, when splitting a data-set into testing and training sets, the goal is to ensure that no data is shared between the two. This is because the test set’s purpose is to simulate real-world, unseen data. However, when evaluating a model, we do have full access to both our train and test sets, so it is up to us to ensure that no data in the training set is present in the test set.\n",
        "\n",
        "In our case, since we want to classify reviews, we have **not to use** test reviews in **text vectorization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-qdVfWagEu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0277a36b-45a1-4b2e-da31-b05115beb454"
      },
      "source": [
        "# save features and targets from the 'data'\n",
        "features, targets = data['text'], data['category_id']\n",
        "\n",
        "train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "        features, targets,\n",
        "        train_size=0.8,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        shuffle = True,\n",
        "        stratify=targets\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 321 ms (started: 2022-10-04 10:38:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKuPtn1VoxnX"
      },
      "source": [
        "# Build the Train & Test TensorFlow Datasets\n",
        "\n",
        "First, we create **TensorFlow Datasets** from the raw Train Dataframe for further processing.\n",
        "\n",
        "Note that:\n",
        "1. **X**: input (text/reviews)\n",
        "2. **y**: target value (categories/topics/class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swV1vlGKocG_"
      },
      "source": [
        "**Observe that** we have **reviews in text** as input and **categories (topics) in integer** as target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E5ba9Gjv0lb",
        "outputId": "c591b0ff-296a-41fc-c1b8-c309a8529118"
      },
      "source": [
        "train_features.values[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Eker Kazandibi Tadı Çok Kötü,Eker kazandibinin tadı sadece tereyağ yiyormuşum gibi ve içinde garip parçacıklar var yenilecek gibi değil. Son kullanma tarihi geçmemesine rağmen bu şekilde mide bulandırıcı bir tadı var. Bence yetkililer önce yemeli sonra insanlara sunmalı. Acilen düzeltilmeli!Devamını oku',\n",
              "       \"Samsung Curved TV Arızası,05.05.2016 tarihinde satın aldığımız 48JU6570 4K UHD televizyonda beyaz lekeler olmak sureti ile en son görüntü tamamen kayboldu. Servis neredeyse yeni bir TV ücreti ile tamir sunuyor. 3 yılda 5000₺ değerinde bir TV'min çöp olması hiç hoş bir durum değil. Curve TV mağduriyetleri sayısız. Samsung'un ...Devamını oku\",\n",
              "       \"Türk Eczacılar Birliği Kırıkkale Eczaneler Maske Vermiyor,Kırıkkale'de ve her yerde eczanelerin maskeyi bedava vereceği satmanın yasak olduğunu biliyoruz. Ama biz eczaneye gittiğimizde maske istediğimizde vermiyorlar. Parayla satın almak istediğimizde de yasak diyorlar. Mecbur evden çıkmamız gerekiyor. Gerekli ilgilenenler bu konuya en kısa sürede el atarl...Devamını oku\",\n",
              "       'Kablo TV Kutusu Seç İzle Sorunu,Kablo TV kutularının altyapısı olmadan bunların satışını yaptılar bize kullanamadığımız özelliklere para verip duruyoruz Seç İzle özelliği devamlı sorun yaratıyor yetkili yerleri arıyorum geliyorlar 1 gün düzgün ertesi gün tekrar aynı bu özellikleri kullanamayacaksak ne anlamı var ayda 100 TL ödemen...Devamını oku',\n",
              "       'Tefal Garanti Süresinin Bitmesini Mi Bekliyorsunuz?,\"Garanti süresi içinde bozulan ürünümü yetkili servisiniz olan Ayser firmasına verdim. 1 ay olmasına rağmen olumlu ya da olumsuz hiçbir yanıt alamadım. Ben arayıp sorduğumda telefona yanıt veren Ö** bey sanki ilk kez bozuk ürün almış gibi yanıtlar verdi. Bilmiyorum, gelir mi gelmez mi belli değil vs....Devamını oku\"'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.72 ms (started: 2022-10-04 10:38:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJTyncnJwOe9",
        "outputId": "6881a6f9-7326-46f5-d848-f71743799e99"
      },
      "source": [
        "train_targets.values[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11,  6, 26, 20, 19], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.68 ms (started: 2022-10-04 10:38:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMwYNTks2WZS"
      },
      "source": [
        "## Prepare TensorFlow Datasets\n",
        "\n",
        "We convert the data stored in Pandas Data Frame into  a data stored in TensorFlow Data Set as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyNh1kCskjVY",
        "outputId": "e0b30875-9fc0-4a0c-d6e4-1660d7fbed29"
      },
      "source": [
        "# train X & y\n",
        "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_features.values, tf.string)\n",
        ") \n",
        "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_targets.values, tf.int64),\n",
        "\n",
        ") \n",
        "# test X & y\n",
        "test_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_features.values, tf.string)\n",
        ") \n",
        "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_targets.values, tf.int64),\n",
        "\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.57 s (started: 2022-10-04 10:38:19 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9oAjlqipore"
      },
      "source": [
        "## Decide the dictionary size and the review size\n",
        "\n",
        "For preprocessing the text, we need to decide the **dictionary (vocabulary) size** and the **review (text) length**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2frnPx9C4wE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cc02bc-b8e4-4966-ac58-ef93224d7e73"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20K words\n",
        "max_len = 50  # Maximum review (text) size in words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 704 µs (started: 2022-10-04 10:38:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1tSOhYdZeX"
      },
      "source": [
        "# **PART C: USE KERAS TEXT VECTORIZATION LAYER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiGIXHdKnpBX"
      },
      "source": [
        "# 8. PREPROCESS THE TEXT WITH THE KERAS `TEXTVECTORIZATION` LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pK1RrG8rROz"
      },
      "source": [
        "\n",
        "\n",
        "## 8.1. Define your own `custom_standardization` function\n",
        "First, I define a function which will preprocess the given text.\n",
        "The `custom_standardization` function will convert the given string to a standart form by transforming the input applying several updates:\n",
        "* convert all characters to lowercase\n",
        "* remove special symbols, extra spaces, html tags, digits, and puctuations\n",
        "* remove stop wrods\n",
        "* replace the special Turkish letters with the corresponding English letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLdLF0qQBH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb402a4-c635-4ee6-9bfc-7c1093e0a6c3"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "    no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
        "    no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
        "    no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
        "    no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
        "    no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
        "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
        "    #remove stop words\n",
        "    no_stop_words = ' '+no_punctuations+ ' '\n",
        "    for each in tr_stop_words.values:\n",
        "      no_stop_words = tf.strings.regex_replace(no_stop_words, ' '+each[0]+' ' , r\" \")\n",
        "    no_extra_space = tf.strings.regex_replace(no_stop_words, \" +\",\" \")\n",
        "    #remove Turkish chars\n",
        "    no_I = tf.strings.regex_replace(no_extra_space, \"ı\",\"i\")\n",
        "    no_O = tf.strings.regex_replace(no_I, \"ö\",\"o\")\n",
        "    no_C = tf.strings.regex_replace(no_O, \"ç\",\"c\")\n",
        "    no_S = tf.strings.regex_replace(no_C, \"ş\",\"s\")\n",
        "    no_G = tf.strings.regex_replace(no_S, \"ğ\",\"g\")\n",
        "    no_U = tf.strings.regex_replace(no_G, \"ü\",\"u\")\n",
        "\n",
        "    return no_U"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.84 ms (started: 2022-10-04 10:38:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2L0siQkzHBI"
      },
      "source": [
        "Quickly verify that `custom_standardization` works: try it on a sample Turkish input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M2IQsBBuy-a",
        "outputId": "8372a342-a28f-4228-9ee9-d94e7007772a"
      },
      "source": [
        "input_string = \"Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\"\n",
        "print(\"input:  \", input_string)\n",
        "output_string= custom_standardization(input_string)\n",
        "print(\"output: \", output_string.numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:   Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\n",
            "output:   bu issiz oglenleyin de pijamali hasta ve i̇ste yagiz sofore cabucak guvendi \n",
            "time: 37.8 ms (started: 2022-10-04 10:38:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkigEVBvjqmk"
      },
      "source": [
        "## 8.2. Configure the Keras `TextVectorization` layer\n",
        "\n",
        "To preprocess the text, I will use the Keras `TextVectorization` layer. \n",
        "\n",
        "```python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    ngrams=None,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=None,\n",
        "    pad_to_max_tokens=False,\n",
        "    vocabulary=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "The Keras `TextVectorization` layer processes each example in the dataset as follows:\n",
        "\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "\n",
        "2. Split each example into substrings (usually words)\n",
        "\n",
        "3. Recombine substrings into tokens (usually ngrams)\n",
        "\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI50-CQ27X-G"
      },
      "source": [
        "Let's build our `TextVectorization` layer by providing:\n",
        "\n",
        "1. The `custom_standardization()` function for the `standardize` method (callable).\n",
        "2. The `vocab_size` as the `max_tokens` number: The `max_tokens` is the maximum size of the vocabulary that will be created from the dataset. If `None`, there is no cap on the size of the vocabulary. Note that this vocabulary contains 1 **OOV (Out Of Vocabulary)** token, so the effective number of tokens is (max_tokens - 1 - (1 if output_mode == \"int\" else 0)).\n",
        "3. The `int` keyword as the `output_mode`: Optional specification for the **output** of the layer. Values can be \n",
        "* \"**int**\", \n",
        "* \"**multi_hot**\", \n",
        "* \"**count**\" or \n",
        "* \"**tf_idf**\", \n",
        "\n",
        "Configuring the layer as follows: \n",
        "* \"**int**\": Outputs integer indices, one integer index per split string token. When output_mode == \"int\", 0 is reserved for masked locations; this reduces the vocab size to max_tokens - 2 instead of max_tokens - 1.\n",
        "\n",
        "* \"**multi_hot**\": Outputs a single int array per batch, of either vocab_size or max_tokens size, containing 1s in all elements where the token mapped to that index exists at least once in the batch item. \n",
        "\n",
        "* \"**count**\": Like \"multi_hot\", but the int array contains a count of the number of times the token at that index appeared in the batch item. \n",
        "\n",
        "* \"**tf_idf**\": Like \"multi_hot\", but the TF-IDF algorithm is applied to find the value in each token slot. \n",
        "\n",
        "For \"**int**\" output, any shape of input and output is supported. \n",
        "\n",
        "For **all other output modes**, currently only **rank 1 inputs** (and rank 2 outputs after splitting) are supported. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-8JyLj68M5j"
      },
      "source": [
        "4. output_sequence_length=max_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbdKK8Uc4ko-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32dc67e3-aa7e-4d19-cfc6-74285708352c"
      },
      "source": [
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size+2,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_len,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 23.2 ms (started: 2022-10-04 10:39:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsc8CNBeldYR"
      },
      "source": [
        "## 8.3. Adapt the Keras `TextVectorization` layer with the **training** data set, (not test data set!) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNSv_ZM9lTKe"
      },
      "source": [
        "`TextVectorization` preprocessing layer has an internal state that can be computed based on a sample of the training data. That is, `TextVectorization` holds a **mapping** between **string** tokens and integer **indices**.\n",
        "\n",
        "Thus, we will ***adopt*** `TextVectorization` preprocessing layer **ONLY** to the **training** data.\n",
        "\n",
        "\n",
        "**Please note that:** To prevent and data leak, we **DO NOT** adopt `TextVectorization` preprocessing layer to the **whole** (***train & test***) data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8nd1qGXwy-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c41a77e-762c-40bf-bf0f-f58c54dd646b"
      },
      "source": [
        "vectorize_layer.adapt(train_features)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 22s (started: 2022-10-04 10:39:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOyv5s64AD1J"
      },
      "source": [
        "Let's see some example conversions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQpBexwy5Fsy",
        "outputId": "26591267-ff1a-43e9-a7dc-90d0f3816ec7"
      },
      "source": [
        "print(\"vocab has the \", len(vocab),\" entries\")\n",
        "print(\"vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \", vocab[word])\n",
        "\n",
        "for X in train_features[:2]:\n",
        "  print(\" Given raw data: \" )\n",
        "  print(X)\n",
        "  tokenized = vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers: \" )\n",
        "  print (tokenized)\n",
        "  print(\" Text after Tokenized and Transformed: \")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ vocab[each]\n",
        "  print(transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab has the  20002  entries\n",
            "vocab has the following first 10 entries\n",
            "0  represents the word:  \n",
            "1  represents the word:  [UNK]\n",
            "2  represents the word:  ve\n",
            "3  represents the word:  bu\n",
            "4  represents the word:  de\n",
            "5  represents the word:  da\n",
            "6  represents the word:  ne\n",
            "7  represents the word:  tl\n",
            "8  represents the word:  gun\n",
            "9  represents the word:  urun\n",
            " Given raw data: \n",
            "Eker Kazandibi Tadı Çok Kötü,Eker kazandibinin tadı sadece tereyağ yiyormuşum gibi ve içinde garip parçacıklar var yenilecek gibi değil. Son kullanma tarihi geçmemesine rağmen bu şekilde mide bulandırıcı bir tadı var. Bence yetkililer önce yemeli sonra insanlara sunmalı. Acilen düzeltilmeli!Devamını oku\n",
            " Tokenized and Transformed to a vector of integers: \n",
            "tf.Tensor(\n",
            "[[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "   9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "     14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Text after Tokenized and Transformed: \n",
            " eker [UNK] tadi kotu eker [UNK] tadi tereyag [UNK] ve icinde garip parcaciklar var yenilecek son kullanma tarihi gecmemesine bu sekilde mide bulandirici tadi var bence yetkililer [UNK] insanlara [UNK] acilen duzeltilmeli                  \n",
            " Given raw data: \n",
            "Samsung Curved TV Arızası,05.05.2016 tarihinde satın aldığımız 48JU6570 4K UHD televizyonda beyaz lekeler olmak sureti ile en son görüntü tamamen kayboldu. Servis neredeyse yeni bir TV ücreti ile tamir sunuyor. 3 yılda 5000₺ değerinde bir TV'min çöp olması hiç hoş bir durum değil. Curve TV mağduriyetleri sayısız. Samsung'un ...Devamını oku\n",
            " Tokenized and Transformed to a vector of integers: \n",
            "tf.Tensor(\n",
            "[[  137 12662    47   415    13    89   186  9719  3811   408  1576   703\n",
            "      1    37    60   950  2413    39    83    47   119   453 10313  1961\n",
            "    754  3514    47  7960  1048   222    42   307     1    47     1  8124\n",
            "    137   533     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Text after Tokenized and Transformed: \n",
            " samsung curved tv arizasi tarihinde satin aldigimiz uhd televizyonda beyaz lekeler olmak [UNK] en son goruntu kayboldu servis yeni tv ucreti tamir sunuyor yilda ₺ degerinde tv min cop olmasi hic durum [UNK] tv [UNK] sayisiz samsung un            \n",
            "time: 308 ms (started: 2022-10-04 10:41:45 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2Jm91uo8GL9",
        "outputId": "d3232ea9-7369-4ea7-c405-1ec97a9ddeb4"
      },
      "source": [
        "vocab[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 've', 'bu', 'de']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.07 ms (started: 2022-10-04 10:41:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwLTEYa7a2aD"
      },
      "source": [
        "## 8.4. Save & Upload TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9geoLkTlzJD"
      },
      "source": [
        "Due to the facts that adapting the Keras `TextVectorization` layer on a large text dataset takes considerable amount of time and porting the adapted layer to a different deployment environment is a high possibility, it is good to know how to save and load it.\n",
        "\n",
        "How to save a Keras `TextVectorization` layer? \n",
        "\n",
        "[There are currently 2 ways of doing it](https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow):\n",
        "* save the Keras `TextVectorization` layer in a Keras Model\n",
        "* save the Keras `TextVectorization` layer as a pickle file.\n",
        "\n",
        "In this tutorial, I will use the first approach as it is native to the TF/Keras environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhXnpHRDntyl"
      },
      "source": [
        "### 8.4.1. Ensure that you are on the correct directory path :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHwRco9l89C9",
        "outputId": "cabae875-3bec-42a3-b36c-785ee9886c3c"
      },
      "source": [
        "%cd ../models/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '../models/'\n",
            "/content/gdrive/MyDrive/kaggle\n",
            "id_to_category.pkl  stop_words_turkish.txt  \u001b[0m\u001b[01;34mvectorize_layer_model\u001b[0m/\n",
            "kaggle.json         ticaret-yorum.csv\n",
            "time: 156 ms (started: 2022-10-04 10:41:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsxmgbFn33v"
      },
      "source": [
        "### 8.4.2. Create a temporary Keras `model` by adding the adapted Keras `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRvfjtqQa8Wa",
        "outputId": "cb954c3e-c6d1-4e7a-a4dd-bb887c0952b8"
      },
      "source": [
        "# Create model.\n",
        "vectorize_layer_model = tf.keras.models.Sequential()\n",
        "vectorize_layer_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "vectorize_layer_model.add(vectorize_layer)\n",
        "vectorize_layer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 209 ms (started: 2022-10-04 10:41:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9MlEks0oG9S"
      },
      "source": [
        "## 8.4.3. Save the temporary model including the adapted Keras `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA0p_Nz6kAyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed13ff4d-1828-4b6e-f11e-f5626fbde609"
      },
      "source": [
        "filepath = \"vectorize_layer_model\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.07 ms (started: 2022-10-04 10:41:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5CCEtKHkEEU",
        "outputId": "7f40da4b-a21e-4eec-aa59-a0ce06ee8662"
      },
      "source": [
        "vectorize_layer_model.save(filepath, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.12 s (started: 2022-10-04 10:41:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYEdEVkY6K27",
        "outputId": "0ab44f47-b254-4339-8828-434560ca183f"
      },
      "source": [
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_to_category.pkl  stop_words_turkish.txt  \u001b[0m\u001b[01;34mvectorize_layer_model\u001b[0m/\n",
            "kaggle.json         ticaret-yorum.csv\n",
            "time: 152 ms (started: 2022-10-04 10:41:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMRIAizqoQt5"
      },
      "source": [
        "### 8.4.4. Load the `vectorize_layer_model` back to chek if saving is succesfull"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-BtuFg-5-yL",
        "outputId": "aa1146a1-b80b-463a-9307-2cfc6e10b67c"
      },
      "source": [
        "loaded_vectorize_layer_model = tf.keras.models.load_model(filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.24 s (started: 2022-10-04 10:42:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06p5ZMh_ofxn"
      },
      "source": [
        "### 8.4.5 Retrieve the **loaded** Keras `TextVectorization` layer\n",
        "\n",
        "Here, you have 2 options:\n",
        "* use the `loaded_model.predicted()` method to use the Keras `TextVectorization` layer, or\n",
        "* get the Keras `TextVectorization` layer out of the `loaded_model` as below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMp7Hu7ipNSJ",
        "outputId": "2fc17bb3-8f56-44b7-c1c9-517587505b34"
      },
      "source": [
        "loaded_vectorize_layer = loaded_vectorize_layer_model.layers[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 539 µs (started: 2022-10-04 10:42:20 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfsI9cVipSUD"
      },
      "source": [
        "### 8.4.6. Compare the original and loaded `TextVectorization` layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk2CVBAjcsEx",
        "outputId": "7f8e1a8b-8503-461b-b565-67da701c1d02"
      },
      "source": [
        "loaded_vocab=loaded_vectorize_layer.get_vocabulary()\n",
        "print(\"original vocab has the \", len(vocab),\" entries\")\n",
        "print(\"loaded vocab has the   \", len(loaded_vocab),\" entries\")\n",
        "print(\"loaded vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \")\n",
        "  print(vocab[word], \" in original vocab\")\n",
        "  print(loaded_vocab[word], \" in loaded vocab\")\n",
        "for X in train_features[:1]:\n",
        "  print(\" Given raw data: \" )\n",
        "  print(X)\n",
        "\n",
        "  tokenized = vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the original vectorize layer:\" )\n",
        "  print (tokenized)\n",
        "\n",
        "  tokenized = loaded_vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the loaded vectorize layer:\" )\n",
        "  print (tokenized)\n",
        "  \n",
        "  tokenized = loaded_vectorize_layer_model.predict(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the loaded_vectorize_layer_model:\" )\n",
        "  print (tokenized)\n",
        "\n",
        "  print(\" Text after Tokenized and Transformed by the original vectorize layer:: \")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ vocab[each]\n",
        "  print(transformed)\n",
        "\n",
        "  print(\" Text after Tokenized and Transformed by the loaded vectorize layer:\")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ loaded_vocab[each]\n",
        "  print(transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original vocab has the  20002  entries\n",
            "loaded vocab has the    20002  entries\n",
            "loaded vocab has the following first 10 entries\n",
            "0  represents the word: \n",
            "  in original vocab\n",
            "  in loaded vocab\n",
            "1  represents the word: \n",
            "[UNK]  in original vocab\n",
            "[UNK]  in loaded vocab\n",
            "2  represents the word: \n",
            "ve  in original vocab\n",
            "ve  in loaded vocab\n",
            "3  represents the word: \n",
            "bu  in original vocab\n",
            "bu  in loaded vocab\n",
            "4  represents the word: \n",
            "de  in original vocab\n",
            "de  in loaded vocab\n",
            "5  represents the word: \n",
            "da  in original vocab\n",
            "da  in loaded vocab\n",
            "6  represents the word: \n",
            "ne  in original vocab\n",
            "ne  in loaded vocab\n",
            "7  represents the word: \n",
            "tl  in original vocab\n",
            "tl  in loaded vocab\n",
            "8  represents the word: \n",
            "gun  in original vocab\n",
            "gun  in loaded vocab\n",
            "9  represents the word: \n",
            "urun  in original vocab\n",
            "urun  in loaded vocab\n",
            " Given raw data: \n",
            "Eker Kazandibi Tadı Çok Kötü,Eker kazandibinin tadı sadece tereyağ yiyormuşum gibi ve içinde garip parçacıklar var yenilecek gibi değil. Son kullanma tarihi geçmemesine rağmen bu şekilde mide bulandırıcı bir tadı var. Bence yetkililer önce yemeli sonra insanlara sunmalı. Acilen düzeltilmeli!Devamını oku\n",
            " Tokenized and Transformed to a vector of integers by the original vectorize layer:\n",
            "tf.Tensor(\n",
            "[[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "   9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "     14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Tokenized and Transformed to a vector of integers by the loaded vectorize layer:\n",
            "tf.Tensor(\n",
            "[[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "   9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "     14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Tokenized and Transformed to a vector of integers by the loaded_vectorize_layer_model:\n",
            "[[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "   9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "     14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]]\n",
            " Text after Tokenized and Transformed by the original vectorize layer:: \n",
            " eker [UNK] tadi kotu eker [UNK] tadi tereyag [UNK] ve icinde garip parcaciklar var yenilecek son kullanma tarihi gecmemesine bu sekilde mide bulandirici tadi var bence yetkililer [UNK] insanlara [UNK] acilen duzeltilmeli                  \n",
            " Text after Tokenized and Transformed by the loaded vectorize layer:\n",
            " eker [UNK] tadi kotu eker [UNK] tadi tereyag [UNK] ve icinde garip parcaciklar var yenilecek son kullanma tarihi gecmemesine bu sekilde mide bulandirici tadi var bence yetkililer [UNK] insanlara [UNK] acilen duzeltilmeli                  \n",
            "time: 526 ms (started: 2022-10-04 10:42:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PopPGDc4p0fZ"
      },
      "source": [
        "As you see above, we succesfully saved and loaded the *adapted* Keras `TextVectorization` layer!\n",
        "\n",
        "We can continue to the TensorFlow datapipeline with the **adapted** Keras `TextVectorization` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IuA1NfTflVVh",
        "outputId": "9f02a869-503a-4a86-bd68-cb14c6efbe82"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/kaggle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.83 ms (started: 2022-10-04 10:42:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp0qIlQf0yym"
      },
      "source": [
        "# 9. APPLY KERAS `TEXTVECTORIZATION` TO TRAIN & TEST DATA SETS \n",
        "\n",
        "We can define a function to apply the Keras `TextVectorization` on a given string as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1KFgd61PQs",
        "outputId": "4c48fbc0-90fb-4d78-eb4a-290f4dcd904c"
      },
      "source": [
        "def convert_text_input(sample):\n",
        "    text = sample\n",
        "    text = tf.expand_dims(text, -1)  \n",
        "    #return tf.squeeze(vectorize_layer(text))\n",
        "    return tf.squeeze(loaded_vectorize_layer(text)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 855 µs (started: 2022-10-04 10:42:45 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0t4sIVH12qk"
      },
      "source": [
        "We use the TensorFlow `tf.data` API (TF Data Pipeline) `map()` funtion to apply `convert_text_input()` on every sample in the `text` column (reviews) of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESBeHt831vwc",
        "outputId": "71dceba7-4976-4458-9a03-096aabe515f9"
      },
      "source": [
        "# Train X\n",
        "train_text_ds = train_text_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# Test X\n",
        "test_text_ds = test_text_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 454 ms (started: 2022-10-04 10:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6q3pkSZ2c2E"
      },
      "source": [
        "Let's see the converted/encoded texts (reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVld41qV2kGZ",
        "outputId": "de2acd0d-5397-404a-edfc-389887e46637"
      },
      "source": [
        "for each in train_text_ds.take(3):\n",
        "  print(each)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "  9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "    14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  137 12662    47   415    13    89   186  9719  3811   408  1576   703\n",
            "     1    37    60   950  2413    39    83    47   119   453 10313  1961\n",
            "   754  3514    47  7960  1048   222    42   307     1    47     1  8124\n",
            "   137   533     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  326  9229  4056  6028 14115  1001   321  6028     4     2    23   518\n",
            "     1 10326  1407 16689     1  2102    43  8347  6932  1446  1001  3311\n",
            "   899  4019    89   142  3311     4  2102   265  2530  1447 18603   413\n",
            "   430     1     3  4237    37   178   485   593     1     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "time: 146 ms (started: 2022-10-04 10:42:58 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHvbhCZk2rsk"
      },
      "source": [
        "10. GENERATE THE TRAIN SET BY COMBINING X & Y:\n",
        "* **X**: the preprocessed & encoded reviews \n",
        "* **y**: encoded categories) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOYpCBmV2xTk",
        "outputId": "e979bb83-a345-4702-e732-bf041c74739a"
      },
      "source": [
        "train_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            train_text_ds,\n",
        "            train_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.24 ms (started: 2022-10-04 10:43:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS6Nje1IJi_D"
      },
      "source": [
        "Similarly, let's bundle test data sets as a single data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQLHIYOJrng",
        "outputId": "351ef911-a7a4-45f0-c92f-0d5b52ee6337"
      },
      "source": [
        "test_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            test_text_ds,\n",
        "            test_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.35 ms (started: 2022-10-04 10:43:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opoknhKL27Lr"
      },
      "source": [
        "We can see the result of the **Text Vectorization** in the **Data Pipelining** as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdqgVuiw3EOM",
        "outputId": "29845a5f-a25b-4889-d0bc-347c4ac040e8"
      },
      "source": [
        "for X,y in train_ds.take(1):\n",
        "  print(\"input (review) X.shape: \", X.shape)\n",
        "  print(\"output (category) y.shape: \", y.shape)\n",
        "  print(\"input (review) X: \", X)\n",
        "  print(\"output (category) y: \",y)\n",
        "  input = \" \".join([vocab[_] for _ in np.squeeze(X)])\n",
        "  output = id_to_category[y.numpy()]\n",
        "  print(\"X: input (review) in text: \" , input)\n",
        "  print(\"y: output (category) in text: \" , output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (review) X.shape:  (50,)\n",
            "output (category) y.shape:  ()\n",
            "input (review) X:  tf.Tensor(\n",
            "[ 9027     1   642    80  9027     1   642  9613     1     2    76  2585\n",
            "  9623    14 19558    60   836   216  7484     3    35  2884  7771   642\n",
            "    14  1476  2131     1  1643     1   839 18588     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "output (category) y:  tf.Tensor(11, shape=(), dtype=int64)\n",
            "X: input (review) in text:  eker [UNK] tadi kotu eker [UNK] tadi tereyag [UNK] ve icinde garip parcaciklar var yenilecek son kullanma tarihi gecmemesine bu sekilde mide bulandirici tadi var bence yetkililer [UNK] insanlara [UNK] acilen duzeltilmeli                  \n",
            "y: output (category) in text:  gida\n",
            "time: 144 ms (started: 2022-10-04 10:43:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwBWJ_vQ3Q9d"
      },
      "source": [
        "# 11. FINALIZE TENSORFLOW DATA PIPELINE\n",
        "Finalize TensorFlow Data Pipeline by setting necessary parameters for batching, shuffling , and optimizing as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lODphjHX3STK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e27322b-610b-4a84-a871-bde3d3f6b52c"
      },
      "source": [
        "batch_size = 64\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "buffer_size= train_ds.cardinality().numpy()\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.87 ms (started: 2022-10-04 10:43:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R4pk6MkUWCJ",
        "outputId": "f8dec50c-3e5a-4d9b-82b4-e8f1b2747a17"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(64,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.65 ms (started: 2022-10-04 10:43:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQcMeCgkd4t6"
      },
      "source": [
        "# **PART D: BUILD AN END-TO-END MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD0PlXFiNSpa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67BIYNoP0E6v"
      },
      "source": [
        "# 12. Create a Classification Model\n",
        "\n",
        "For the sake of demonstration of the Keras `TextVectorization` layer, let's build a very simple model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2hK2ReMnHfz",
        "outputId": "7e7ea6d7-ba32-404a-d687-681cea92197d"
      },
      "source": [
        "def create_model():\n",
        "    inputs_tokens = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding_layer = layers.Embedding(vocab_size, 256)\n",
        "    x = embedding_layer(inputs_tokens)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(32)(x) #no of input layers\n",
        "    model = keras.Model(inputs=inputs_tokens, outputs=outputs)\n",
        "    \n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric_fn  = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metric_fn)  \n",
        "    \n",
        "    return model\n",
        "my_model=create_model()\n",
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 50, 256)           5120000   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                409632    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 460 ms (started: 2022-10-04 10:43:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtGgE1FxsRb"
      },
      "source": [
        "# 13. Train the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhiuaXoCxynm",
        "outputId": "16a9a8f7-349c-418a-afa7-faad1512f34b"
      },
      "source": [
        "my_model.fit(train_ds, verbose=1, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5391/5391 [==============================] - 185s 5ms/step - loss: 0.3027 - sparse_categorical_accuracy: 0.9309\n",
            "Epoch 2/3\n",
            "5391/5391 [==============================] - 27s 5ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 3/3\n",
            "5391/5391 [==============================] - 26s 5ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbd2c60550>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4min 12s (started: 2022-10-04 10:43:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGjnwq6SyUX-",
        "outputId": "17006041-7b5f-46e5-e479-ae84ec6e2f34"
      },
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1347/1347 [==============================] - 41s 2ms/step - loss: 0.2569 - sparse_categorical_accuracy: 0.9494\n",
            "Train accuracy:  0.9493666291236877\n",
            "time: 41.5 s (started: 2022-10-04 10:47:45 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zlRPwlBx-L_"
      },
      "source": [
        "# 14. An End-To-End Classification Model\n",
        "\n",
        "Pay attention that the above model is expected to receive batches of integer tensors as input:\n",
        "\n",
        "```\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " input_3 (InputLayer)        [(None, 50)]              0         \n",
        "```\n",
        "Thus, you can NOT supply raw data (some text) to the model for prediction. TensorFlow/Keras would generate error message as below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "budX5NPuMOS5"
      },
      "source": [
        "```python\n",
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "\n",
        "predictions=my_model.predict(raw_data)\n",
        "\n",
        "ValueError: in user code: Exception encountered when calling layer \"model\" (type Functional).\n",
        "    \n",
        "    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 12800, but received input with shape (None, 256)\n",
        "    \n",
        "    Call arguments received:\n",
        "      • inputs=tf.Tensor(shape=(None,), dtype=string)\n",
        "      • training=False\n",
        "      • mask=None\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxyclVcM4hB"
      },
      "source": [
        "However, sometimes it a big advantage if we can design a model which accepts raw data as input, then, process the data by itself.\n",
        "\n",
        "For example such a model can be easily exported different platforms/environments without the need of exporting the preprocess code!\n",
        "\n",
        "Therefore, Keras provides [several Preprocessing Layers](https://keras.io/api/layers/preprocessing_layers/) so that we can integrate preprocessing logic as a layer into a Keras model.\n",
        "\n",
        "After then, we can export such models and use any other platforms without re-writing preprocessing code on the exported platforms/environments.\n",
        "\n",
        "This kind of models can be called **End-To-End Models**. That is, an **End-To-End model** can accept Raw Input Data and preprocess it by itself.\n",
        "\n",
        "**What could be Raw Data? **\n",
        "\n",
        "It could be:\n",
        "* text\n",
        "* image\n",
        "* structure data\n",
        "* etc.\n",
        "\n",
        "Let's create an **End-To-End Classification Model** by integrating the **adapted** Keras `TextVectorization` layer into the **trained model** as **the first layer**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQgRdLHXL5jT"
      },
      "source": [
        "You can create an **End-To-End Model** either by:\n",
        "* Keras Sequential API, or\n",
        "* Keras Functional API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyHOxb210keN"
      },
      "source": [
        "## 14.1. Create an End-To-End Model with Keras Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IhG3stoIAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596ca50d-f267-415c-c521-c6b7764a9a98"
      },
      "source": [
        "end_to_end_model = tf.keras.Sequential([\n",
        "  keras.Input(shape=(1,), dtype=\"string\"),\n",
        "  vectorize_layer,\n",
        "  my_model,\n",
        "  layers.Activation('softmax')\n",
        "])\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "end_to_end_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model (Functional)          (None, 32)                5529632   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 203 ms (started: 2022-10-04 11:02:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaGKzNN0o3Q"
      },
      "source": [
        "## 14.2. Create an End-To-End Model with Keras Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkQNhhD8296z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc298ed1-ab4a-46bc-d462-f142665420a5"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorize_layer(inputs)\n",
        "outputs = my_model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "end_to_end_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model (Functional)          (None, 32)                5529632   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 217 ms (started: 2022-10-04 11:02:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw0C0IvF28T9"
      },
      "source": [
        "## 14.3. Test the End-to-End model with Raw (Text) Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmydvPGg3adQ",
        "outputId": "5f549f38-2576-430c-ae4b-de1085856a60"
      },
      "source": [
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "predictions=end_to_end_model.predict(raw_data)\n",
        "print(id_to_category[np.argmax(predictions[0])])\n",
        "print(id_to_category[np.argmax(predictions[1])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alisveris\n",
            "ulasim\n",
            "time: 404 ms (started: 2022-10-04 11:02:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln_ydyp5yob6",
        "outputId": "fad42dea-b461-4b28-d2b1-5037032304ac"
      },
      "source": [
        "loss, accuracy = end_to_end_model.evaluate(test_features,test_targets)\n",
        "print(\"end_to_end_model accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2696/2696 [==============================] - 35s 13ms/step - loss: 2.3971 - accuracy: 0.9494\n",
            "end_to_end_model accuracy:  0.9493751525878906\n",
            "time: 35.1 s (started: 2022-10-04 11:03:01 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhhQiICa7e38"
      },
      "source": [
        "## 14.4. Save the End-to-End model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEsPYR4e4Ihh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69295c3-e7ec-4f6e-fa9b-26ff1c9f7181"
      },
      "source": [
        "end_to_end_model.save(\"end_to_end_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.77 s (started: 2022-10-04 11:06:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KldU3Asf7kHV"
      },
      "source": [
        "## 14.5. Load the End-to-End model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLrdCADgng_D",
        "outputId": "22a62b17-5bba-437a-fa9e-56f5958f430a"
      },
      "source": [
        "#changing the working directory\n",
        "%cd \"/content/gdrive/MyDrive/kaggle\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/kaggle\n",
            "time: 6.25 ms (started: 2022-10-04 11:11:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrKKxntwlHom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2e6f3a-02ff-418f-e308-48083ba2ead2"
      },
      "source": [
        "loaded_end_to_end_model = tf.keras.models.load_model(\"end_to_end_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.82 s (started: 2022-10-04 11:11:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYFqCKDk7mxD"
      },
      "source": [
        "## 14.6. Test the Loaded End-to-End model with Raw (Text) Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtRzOEg6led",
        "outputId": "b930d426-9d53-4882-87bd-39c6befbf7e6"
      },
      "source": [
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "predictions=loaded_end_to_end_model.predict(raw_data)\n",
        "print(id_to_category[np.argmax(predictions[0])])\n",
        "print(id_to_category[np.argmax(predictions[1])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alisveris\n",
            "ulasim\n",
            "time: 353 ms (started: 2022-10-04 11:11:30 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG0pj0Bv3LPW",
        "outputId": "ed2a5dc1-e6cb-4e3f-8108-cc98e82d4e00"
      },
      "source": [
        "loss, accuracy = loaded_end_to_end_model.evaluate(test_features,test_targets)\n",
        "print(\"loaded_end_to_end_model accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2696/2696 [==============================] - 41s 15ms/step - loss: 2.3971 - accuracy: 0.9494\n",
            "loaded_end_to_end_model accuracy:  0.9493751525878906\n",
            "time: 40.9 s (started: 2022-10-04 11:11:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcfd6oqiD9R_"
      },
      "source": [
        "# **PART E: SUMMARY**\n",
        "In this tutorial, we have learned:\n",
        "* What a Keras `TextVectorization` layer is\n",
        "* Why we need to use a Keras `TextVectorization` layer in Natural Languge Processing (NLP) tasks\n",
        "* How to employ a Keras `TextVectorization` layer in Text Preprocessing\n",
        "* How to integrate a Keras `TextVectorization` layer to a trained model\n",
        "* How to save and upload a Keras `TextVectorization` layer and a model with a Keras `TextVectorization` layer\n",
        "* How to integrate a Keras `TextVectorization` layer with TensorFlow Data Pipeline API (`tf.data`)\n",
        "* How to design, train, save, and load an End-to-End model using Keras `TextVectorization` layer\n",
        "\n",
        "All above topics are presented in a **multi-class text classification** context.\n",
        "\n",
        "If you like this tutorial, please follow the Murat Karakaya Akademi [YouTube channel](https://www.youtube.com/c/MuratKarakayaAkademi) and [Medium blog](https://kmkarakaya.medium.com/).\n",
        "\n",
        "**Thank you for your patience!**\n",
        "\n",
        "#Keep Deep Learning :)"
      ]
    }
  ]
}